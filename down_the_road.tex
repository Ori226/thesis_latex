\documentclass[]{report}
\setlength{\parindent}{2em}
\setlength{\parskip}{0.3em}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


% Title Page
\title{fdfdfd}
\author{}


\begin{document}
\maketitle

\begin{abstract}
	In this report I will summarize the work I conducted when trying to use LSTM for  P-300 speller classification task .
\end{abstract}



\section{Motivation}
There are various methods of using machine learning to solve the P-300 speller classification task. None of these methods use Recurrent Neural Network.  

In this report I will describe how I used LSTM for the task.

\subsection{The P-300 speller }
\textit{taken from\cite{CNN_p300}}
The P300 is an event-related potential (ERP) which can be recorded via EEG. The wave corresponds to positive deflection in voltage at a latency about 300 ms in the EEG. In other words, it means that after an event like a flashing light, a deflection in the signal should occur after 300ms. The detection of a P300 wave is equivalent to where the user was looking 300 ms before its detection. In a P300 speller, the main goal is to detect P300 peaks in the EEG accurately and instantly. The accuracy of this detection will unsure high information transfer rate between the user and the machine.



\section{P300 detection - Two types of classification problem}
There exists two types of classification for the problem of P300-based BCI:
\begin{enumerate}
	\item The detection of P300 response. It corresponds to a binary classification: one class represent signals that correspond to a P300 wave (i.e. target), the second class is the opposite (i.e. non-target). 
	\item The character recognition. The output of the previous classification are then combined to classify the main classes of the application ( in this case the character). In the RSVP P-300 speller, the character is supposed to correspond to the intersection of the accumulation of several P300 waves. The best accumulation over multiple trials determines the desired character. 
	
In \cite{Blankertz}, the author focused only on the character recognition task. The first part of each experiment was used for training (i.e. calibration) and the other part were used for testing.

\end{enumerate}
\section{Database}


The database includes 55 channels of EEG recordings from 12 subjects. Each subject is presented with 600 to 700 repetitions of 30 different letters and symbols. In total there is approximately 20,000 samples for each subject, 1/30 of them suppose to contain P300 wave. The interval between each presentation of the stimuli is 116ms.The exact method by which a single sample is extract from the complete EEG recording can be different between the experiments. For more detail, see \cite{Blankertz}.


\section{Network Architecture} \label{netarch}
The network has 4 layers. The first Layer-1 contains 20 or 100 hidden LSTM units. Layer-2 is a simple DropOut layer (i.e. a layer that randomly trun-off the activation of the previous layer in order to help avoid over fitting). Layer-3 is the same as layer -1.All the layers except from the layer-3 receive a sequence of time-series data and return a sequence of time-series. The last layer return the last state of the network and pass layer 4 which contains 2-cells. The two cells represent the probability of a sample to contain P300 (The sum of both cells is 1)

\begin{figure*}[h]
	\includegraphics[width=8cm]{"LSTM architecture -Binary_general"}
	\caption{general structure of the LSTM layer}
	\label{fig:RSVP}
\end{figure*}



\section{Experiments}

During my attempts to use LSTM I conducted several experiments. The experiments shows that LSTM can be used both for the task of P300 identification and character recognition (although some find tuning is required). 

The first experiment is meant to examine whether it possible to detect P-300 wave using LSMT. On the second experiment I used LSTM the for character recognition task.

It is important to emphasis that there is a lot of difference between testing only the P300 detection task and testing the character recognition task. Unlike the P300 detection task, in order to show that LSTM can be used as part of a real P300 BCI system, the LSTM model is always trained only on the first part of the recording (i.e. \cite{Blankertz} calibration phase]). This means that unlike other machine learning task, the model is trained only on 1/3 of the data and test on the other 2/3. Training on the first part of the recording pose another challenge: modelling the intra-subject variance across time (i.e. P300 wave will be very different when recording with a gap of 10 minutes from each other).

\subsection{Experiment 1 P300 detection}

In this experiment, equal number of target and non-target used as for training and testing. Since the number of target samples is only 1/30 from the number of non-target, only part of the non-target were selected randomly from the data. In this experiment, each sample was taken at the stimuli onset (i.e. time 0) until 400ms after it. Each 400ms sample is then normalized using z-score. Each sample is in a matrix of 55X80 (channels X timestamps). For evaluation I used 4-fold cross validation.

The results can be see in  Fig\ref{fig:P300_detection}: and Table. \textit{bottom line: much better than chance}. 

\begin{figure*}[h]
	\includegraphics[width=14cm]{"P300DetectionAccuracy"}
	\caption{P300 detection accuracy per subject}
	\label{fig:P300_detection}
\end{figure*}






\subsection{Experiment 2 character recognition - estimation method}
In here I will give a formal explanation about how a model is being evaluated for the task of character recognition. Unlike P300 identification, , the goal is to identify which one of the 30 sample contains P300. Each round of 30 symbols (i.e. the complete display of the AlphaBet in random order) is called \textit{Block} and is repeated for 10 times. The P300 prediction is then average \textbf{for each letter}.



More formally:

\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\[l\left( i \right) = \arg {\max _c}\left\{ {\sum\limits_{{x_r} \in {\rm{Repetition(c)}}}^{} {f\left( {\mathop x\nolimits_r^c } \right)} |c \in Alphabet} \right\}\]



Where $l(i)$ denote the selected letter after repeating several time on the complete Alphabet and $f(x)$ denote the classifier result for the score.



I conducted a set of experiment to identify how different variation in the training method and the network architecture affect the results. The different variation include:

\begin{enumerate}
	\item \textbf{The period of each sample relative to the stimuli onset}. Two method for were tested: a) beginning exactly from the stimuli onset and ending 400ms after it (\textbf{epo\_0\_400}) and b) start 200ms before the stimuli onset and end 800ms after it (\textbf{epo\_-200\_800}). % maybe some explanation about the motivation
	
	\item \textbf{The number of hidden units} The specification of the  network is describe in section \ref{netarch}. The only difference between the different networks is the number of units in each one of the layers: a) the 20 units in each LSTM layer and b) 100 units in each LSTM layer. % maybe some explanation about the motivation: regularazation vs model strength
	

	
	\item \textbf{The subset of non-target samples used in the experiment}:	a) All the samples were used for the training. It is important to emphasis that since there are a lot more non-target samples, in this method, the training dataset is bias toward the non-target class. b) Only a random subset of non-target samples was selected, such that the number of target and non-target samples in the training will be equal.
	\item \textbf{The sampling rate}: a) using the original sampling rate:200hz method a) reducing the frequency to 25hz by averaging each 8 consecutive 8 samples.
	\item \textbf{The positive samples weight in the loss function}: 
	The original loss function in this experiment is the categorical cross entropy:
	\begin{equation}\label{eq:2}
	H(p,q) = - \sum_x p(x) \log(q(x))
	\end{equation}
	Where \textit{p} denote the sample's true label and \textit{q} the probability score as predicted by the classifier. During training, the goal of is to minimize the term in \eqref{eq:2}. In order to avoid the bias of the toward the bigger group of non-target samples, I have changed the weight of the non-target samples in the loss function:
	\begin{equation}\label{eq:3}
	H(p,q) = - \sum_x w_i*p(x) \log(q(x)) 
	\end{equation}
	Two method were tested a) $w_i$ is 1 both for target and non-target, b) $w_i$ is 50 for the target samples and 1 for the non target.
	
	
\end{enumerate}


%\begin{equation}\label{eq:1}
% \argmax_c f(c)=\{c \mid f(c) = \min_{c} f(x')\}
%\end{equation}
\subsection{classification results}

The 
%
%\subsection{Experiment 2 character recognition version 1}
%In this experiment, I took the same model and the same training method I used in the experiment 1 and use it for the character recognition task.
%The results of this experiment can be seen in figure \textit{TBD}. 
%
%\textit{Experiment details:} The training data was taken from the first 1/3 of the experiment (\cite{Blankertz} calibration phase) and estimate the most probable character at each sequence of repetition as in Eq.ref{eq:1}. The testing was used on last 2/3 of  the experiments as in \cite{Blankertz}.


\section{analyzing using the gradient}
As suggested by \cite{AlexGraveBook}, the gradient of the neural network with respect to the input can be a helpful tool in analysing the network. Input with high gradient value indicate that change in this input will result in a very different output (at least locally). By using these method, it seems that in opposed to non-recurrent network (i.e. feed-forward network), the LSTM network learned to focus only on small temporal interval of the input corresponding to the theory of P300. 


\subsection{Gradient - reminder}
\textit{\textbf{Taken from Wikipedia: } If $f(x1, ..., xn)$ is a differentiable, scalar-valued function, its gradient is the vector whose components are the n partial derivatives of f. It is thus a vector-valued function.}

In our case of $f(x|\theta)$ is the neural network with fixed weights $\theta$ and input $x$. The partial derivatives of $f(x|\theta)$ with respect to $x$ we can be interpreted as how changing each value of $x$ will change the prediction score. It is important not to confuse with the gradient used for the training. While training, the goal is to  optimize the model parameters $\theta$ when \textbf{$x$ is fixed}.


\subsection{The gradient of $f(x|\theta)$ for P300 prediction}
In the case of P300 prediction $x$ is a matrix of $C\times{T}$ ($C$ - number of channels, $T$ the number of time steps) and $f(x|\theta)$ is the neural network with $\theta$ being model's weights. The gradient $\nabla{f(x|\theta)}$ eq.\ref{eq:4} is a matrix in the same size as the input $x$, where the amplitude of each cell reflects its impact on the function value. Cells with high absolute value can be interpreted as the cells that most influence the prediction function.

\begin{equation}\label{eq:4}
\nabla f\left( {x|\theta } \right) = \left[ {\begin{array}{*{20}{c}}
	{\frac{{partial\left( {x|\theta } \right)}}{{dx\left( {{c_1},{t_1}} \right)}}}&{...}&{\frac{{partial\left( {x|\theta } \right)}}{{dx\left( {{c_1},{t_T}} \right)}}}\\
	{...}&{...}&{...}\\
	{\frac{{partial\left( {x|\theta } \right)}}{{dx\left( {{c_C},{t_1}} \right)}}}&{...}&{\frac{{partial\left( {x|\theta } \right)}}{{dx\left( {{c_C},{t_T}} \right)}}}
	\end{array}} \right]
\end{equation}


\subsection{Comparing between the Graident of LSTM and MLP}













\section{What next}





\begin{thebibliography}{9}
	
	\bibitem{CNN_p300} 
	Cecotti, H., Graser, A.	
	\textit{Convolutional neural networks for P300 detection with application to brain-computer interfaces}. 
	IEEE Trans. Pattern Analysis and Machine Intelligence, 2010
	
	\bibitem{Blankertz} 
	Cecotti, H., Graser, A.	
	\textit{Convolutional neural networks for P300 detection with application to brain-computer interfaces}. 
	IEEE Trans. Pattern Analysis and Machine Intelligence, 2010
	
	\bibitem{AlexGraveBook} 
	Graves, Alex.
	\textit{"Supervised sequence labelling." Supervised Sequence Labelling with Recurrent Neural Networks.} Springer Berlin Heidelberg, 2012. 5-13.
	
	
\end{thebibliography}


\end{document}          


